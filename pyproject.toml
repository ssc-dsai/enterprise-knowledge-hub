[project]
name = "enterprise-knowledge-hub"
version = "0.1.0"
description = "Enterprise Knowledge hub to index the documents from various sources"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "accelerate>=1.12.0",
    "bitsandbytes>=0.49.1",
    "fastapi[standard]>=0.124.4",
    "langchain>=1.2.1",
    "llama-cpp-python>=0.3.16",
    "ninja>=1.13.0",
    "pgvector>=0.2.5",
    "pika>=1.3.2",
    "psycopg[binary,pool]>=3.2",
    "sentence-transformers>=5.2.0",
    "torch>=2.9.1",
    "transformers>=4.57.3",
]

[project.optional-dependencies]
# CUDA-specific dependencies that require NVIDIA GPU and CUDA toolkit
cuda = [
    "flash-attn>=2.8.3",
]

[dependency-groups]
dev = [
    "pylint>=4.0.4",
]

[tool.uv]
no-build-isolation-package = ["flash-attn"]

# llama-cpp-python build configuration
#   DOC: https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#installation
#   Uncomment ONE of the following extra-build-variables lines based on your hardware:
#   follow up with (if rebuilding):
#   uv sync --upgrade --no-cache --reinstall-package llama-cpp-python

# CUDA support (NVIDIA GPU)
#extra-build-variables = { llama-cpp-python = { CMAKE_ARGS = "-DGGML_CUDA=on -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc" }}

# Metal support (Apple Silicon)
# extra-build-variables = { llama-cpp-python = { CMAKE_ARGS = "-DGGML_METAL=on" } }

# OpenBLAS support
# extra-build-variables = { llama-cpp-python = { CMAKE_ARGS = "-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" } }